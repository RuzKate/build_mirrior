{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from delta import *\n",
    "import os\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"BuildMirrors\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/kate/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/kate/.ivy2/cache\n",
      "The jars for the packages stored in: /home/kate/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d757f48b-63f0-4bc9-9ccd-5de47c934dc3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 2164ms :: artifacts dl 113ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d757f48b-63f0-4bc9-9ccd-5de47c934dc3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/130ms)\n",
      "24/07/17 13:33:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_logging(start_loading, table_name, id_delta, end_loading):\n",
    "    \"\"\"Процесс логирования\n",
    "\n",
    "        :param start_loading: время начала загрузки дельты\n",
    "        :type start_loading: datetime\n",
    "        :param table_name: наименование обновляемой таблицы\n",
    "        :type table_name: str\n",
    "        :param id_delta: идентификатор дельты\n",
    "        :type id_delta: str\n",
    "        :param end_loading: время завершения загрузки дельты\n",
    "        :type end_loading: datetime\n",
    "    \"\"\"\n",
    "    log_data = [\n",
    "        (start_loading, table_name, id_delta, end_loading)\n",
    "    ]\n",
    "\n",
    "    log_schema = \"start_loading timestamp, table_name string, id_delta string, end_loading timestamp\"\n",
    "\n",
    "    log_df = spark.createDataFrame(log_data, log_schema)\n",
    "\n",
    "    log_df.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode('append') \\\n",
    "        .format('csv') \\\n",
    "        .option('header', True) \\\n",
    "        .save('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_deltas():\n",
    "    \"\"\"Получение обработанных дельт\n",
    "\n",
    "        :rtype: list\n",
    "        :return: список обработанных дельт\n",
    "    \"\"\"\n",
    "    if os.path.isdir('logs'): \n",
    "        logs_df = spark \\\n",
    "                .read \\\n",
    "                .format('csv') \\\n",
    "                .option('header', True) \\\n",
    "                .load('logs')\n",
    "        \n",
    "        processed_deltas = [i[0] for i in logs_df.select('id_delta').collect()]\n",
    "        return processed_deltas\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mirrior(delta_path, table_name, unique_keys):\n",
    "    \"\"\"Построение зеркал\n",
    "\n",
    "        :param delta_path: путь где хранятся дельты\n",
    "        :type delta_path: str\n",
    "        :param table_name: наименование таблицы\n",
    "        :type table_name: str\n",
    "        :param unique_keys: список полей, являющимся уникальным ключём\n",
    "        :type unique_keys: list\n",
    "    \"\"\"\n",
    "    processed_deltas = get_processed_deltas()\n",
    "    delta_directories = sorted(os.listdir(delta_path))\n",
    "\n",
    "    for delta_dir in delta_directories:\n",
    "        if delta_dir not in processed_deltas:\n",
    "            delta_file = os.path.join(delta_path, delta_dir)\n",
    "            \n",
    "            delta_df = spark \\\n",
    "                    .read \\\n",
    "                    .format('csv') \\\n",
    "                    .option('header', True) \\\n",
    "                    .option('sep', ';') \\\n",
    "                    .load(delta_file)\n",
    "            \n",
    "            start_loading = datetime.now()\n",
    "            if not DeltaTable.isDeltaTable(spark, f'spark-warehouse/{table_name}'):\n",
    "                delta_df.write.format('delta').saveAsTable(f'{table_name}')\n",
    "            else:\n",
    "                DeltaTable.forPath(spark, f'spark-warehouse/{table_name}').alias('target').merge(\n",
    "                    delta_df.alias('source'),\n",
    "                    ' AND '.join(f'target.{i} = source.{i}' for i in unique_keys)\n",
    "                ).whenMatchedUpdateAll() \\\n",
    "                .whenNotMatchedInsertAll() \\\n",
    "                .execute()\n",
    "            end_loading = datetime.now()\n",
    "\n",
    "            delta_logging(start_loading, table_name, delta_dir, end_loading)\n",
    "\n",
    "    res_mirror = spark.read.format('delta').load(f'spark-warehouse/{table_name}')\n",
    "    res_mirror.coalesce(1).write \\\n",
    "            .format('csv') \\\n",
    "            .mode('overwrite') \\\n",
    "            .option('header', True) \\\n",
    "            .save(f'mirr_{table_name}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = 'data_deltas'\n",
    "table_name = 'md_account_d'\n",
    "unique_keys = ['ACCOUNT_RK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "build_mirrior(delta_path, table_name, unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      3|2024-07-17 13:41:...|  NULL|    NULL|               MERGE|{predicate -> [\"(...|NULL|    NULL|     NULL|          2|  Serializable|        false|{numTargetRowsCop...|        NULL|Apache-Spark/3.5....|\n",
      "|      2|2024-07-17 13:41:...|  NULL|    NULL|               MERGE|{predicate -> [\"(...|NULL|    NULL|     NULL|          1|  Serializable|        false|{numTargetRowsCop...|        NULL|Apache-Spark/3.5....|\n",
      "|      1|2024-07-17 13:40:...|  NULL|    NULL|               MERGE|{predicate -> [\"(...|NULL|    NULL|     NULL|          0|  Serializable|        false|{numTargetRowsCop...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2024-07-17 13:39:...|  NULL|    NULL|CREATE TABLE AS S...|{partitionBy -> [...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 1, n...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table = DeltaTable.forPath(spark, f'spark-warehouse/{table_name}')\n",
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+--------------------+---------+-----------+-------------+---------+---------+----------------+\n",
      "|DATA_ACTUAL_DATE|DATA_ACTUAL_END_DATE|ACCOUNT_RK|      ACCOUNT_NUMBER|CHAR_TYPE|CURRENCY_RK|CURRENCY_CODE|CLIENT_ID|BRANCH_ID|OPEN_IN_INTERNET|\n",
      "+----------------+--------------------+----------+--------------------+---------+-----------+-------------+---------+---------+----------------+\n",
      "|      15.02.2018|          31.12.2050|     13560|30110810300000008001|        A|         34|          643|       20|      105|               Y|\n",
      "|      21.04.2018|          31.12.2050|     13630|30102810900000002185|        A|         34|          643|       21|      107|            NULL|\n",
      "|      21.04.2018|          31.12.2050|     13811|30221978100000008100|        A|         44|          978|       33|      201|            NULL|\n",
      "|      21.04.2018|          31.12.2050|     13871|30222978200000004100|        P|         44|          978|       63|      105|               Y|\n",
      "|      01.01.2018|          31.12.2050|     13904|30204810500000002001|        A|         34|          643|       37|      201|               Y|\n",
      "|      21.04.2018|          31.12.2050|     13905|30202810900000002001|        A|         34|          643|       12|      107|            NULL|\n",
      "|      01.01.2018|          31.12.2050|     13906|30114756800000051003|        A|         30|          756|       40|      105|            NULL|\n",
      "|      15.02.2018|          31.12.2050|     14136|30220826800838890001|        P|         31|          826|       57|      107|            NULL|\n",
      "|      01.01.2018|          31.12.2050|     14138|30220978800838890001|        P|         44|          978|       24|      105|               Y|\n",
      "|      01.01.2018|          31.12.2050|     17132|30111810000000666001|        P|         34|          643|       19|      101|            NULL|\n",
      "|      15.02.2018|          31.12.2050|     17244|30111810900000672001|        P|         34|          643|       99|      127|            NULL|\n",
      "|      01.01.2018|          31.12.2050|     17434|30111810900000051004|        P|         34|          643|        9|      101|            NULL|\n",
      "|      10.03.2018|          31.12.2050|     17439|30111810500000438001|        P|         34|          643|        4|      203|            NULL|\n",
      "|      10.03.2018|          31.12.2050|     17442|30111810800000565001|        P|         34|          810|       82|      107|               N|\n",
      "|      10.03.2018|          31.12.2050|     18006|30233978800450003001|        A|         44|          978|       71|      201|            NULL|\n",
      "|      10.03.2018|          31.12.2050|     18164|30232810000450001001|        P|         34|          810|       90|      103|            NULL|\n",
      "|      21.04.2018|          31.12.2050|     18165|30232978900450001001|        P|         44|          978|       13|      120|               Y|\n",
      "|      01.01.2018|          31.12.2050|  18849846|30109810500000435003|        P|         34|          643|        5|      201|               Y|\n",
      "|      01.01.2018|          31.12.2050|   1972647|30111810700000908001|        P|         34|          643|        3|      101|            NULL|\n",
      "|      01.01.2018|          31.12.2050|     24656|30114840700000770002|        A|         35|          840|        2|      107|            NULL|\n",
      "+----------------+--------------------+----------+--------------------+---------+-----------+-------------+---------+---------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfv2 = spark.read.format('delta').option('versionAsOf', 3).load(f'spark-warehouse/{table_name}')\n",
    "dfv2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_delta_df = spark.createDataFrame(\n",
    "    [\n",
    "        ('01.01.2018', '31.12.2050', '24656', \n",
    "         '30114840700000770002', 'A', '35', '840', '100', '107', None)\n",
    "    ],\n",
    "    schema='DATA_ACTUAL_DATE string, DATA_ACTUAL_END_DATE string, \\\n",
    "    ACCOUNT_RK string, ACCOUNT_NUMBER string, CHAR_TYPE string, \\\n",
    "    CURRENCY_RK string, CURRENCY_CODE string, CLIENT_ID string, \\\n",
    "    BRANCH_ID string, OPEN_IN_INTERNET string'\n",
    ")\n",
    "\n",
    "new_delta_df.coalesce(1) \\\n",
    "            .write \\\n",
    "            .format('csv') \\\n",
    "            .option('header', True) \\\n",
    "            .option('sep', ';') \\\n",
    "            .save(f'{delta_path}/1004/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
